---
title: "Predicting Unemployment in US"
author: "Carolina Cornejo Castellano, Juan D Mendez, Jasmin Huynh"
date: '`r Sys.Date()`'
output: pdf_document
---

## Intro

## About the Current Population Survey


## Methodology

### Preprocessing

```{r libraries}
library(tidyverse)
library(caret) # for machine learning
library(naniar) # for better handling NA
library(fastDummies) # for 'dummifying' categorical variables
library(factoextra)
library(ROCR) #for plotting AUC
library(janitor) # for standardizing column names
#library(MASS)
library(pROC)
library(klaR)
library(ggplot2)
```


We use three datasets (2015-2017) in order to obtain a larger training dataset. Data cleaning included manually selecting variables based on previous trial/error attempts. We filter keeping only cases where the age is older than 16. 

Then we drop rows where negative values where present. This was the case in the columns 'incp_all' (total income) and 'hrearn' (hourly earnings). For model testing we read the 2018 dataset and apply the same data cleaning steps.

```{r read datasets, select columns and filter}
df2018 <- read.csv("~/SRM II/Challenge/cps_march_2018.csv", sep = ";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)


df2017 <- read.csv("~/SRM II/Challenge/cps_2017.csv", sep = ";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)


df2016 <- read.csv("~/SRM II/Challenge/cps_2016.csv", sep = ";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)


df2015 <- read.csv("~/SRM II/Challenge/cps_2015.csv", sep = ";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)
```

Next we combine 2017, 2016 and 2015 datasets to one single dataframe 'df'. 
The columns 'unem', 'hprsmort', 'hours' and 'fulltimelw' contain missing values. We drop rows where NA's for the dependent variable 'unem' where present because the number of missing cases isn't high compared to the whole dataset.

```{r}
df <- bind_rows(df2017,df2016, df2015) %>% 
  filter(!is.na(unem))
```

The remaining columns that have NA are "hours" (hours worked last week), "fulltimelw" (fulltime schedule last week) and "hprsmort" (presence of home mortgage). First, we assume the NA for "hours" means those cases didn't work last week. Second, we assume that NA for "fulltimelw" and "hprsmort" are negative cases. Based on these assumptions we impute NA with zeros.

```{r}
summary(is.na(df))
```

```{r NA handling}
df[is.na(df)] <- 0
```

We convert categorical variables to factor. 

```{r}
str(df)
df <- df %>% 
  mutate(across(c("wbhaom",
                   "marstat",
                   "clslyr",
                   "ftptlyr", 
                   "female",
                   "forborn", 
                   "citizen",
                   "hprsmort", 
                   "unem",
                   "fulltimelw"), 
                 as.factor))

```
### Exploratory Data Analysis (EDA) 

Here we explore the class proportion of the dependent variable 'unem'.

```{r}
unem_prop <- df %>%
  group_by(unem) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup() %>%
  mutate(label_text = str_glue("n:{n} \n prop:{scales::percent(prop)}"))

unem_prop

```
The proportion table and the plot shows that we have an imbalanced dataset which contains only 3% positive cases for the dependent variable.

```{r}
options(warn = -1)
sam <- theme(plot.background = element_rect(fill="#F5FFFA",color = "darkblue"),
             plot.title = element_text(size=25, hjust=.5),
             axis.title.x = element_text(size=25, color = "black"),
              axis.title.y = element_text(size=25, color = "black"),
             axis.text.x = element_text(size=20),
             axis.text.y = element_text(size=20),
            legend.position = "top",
            legend.text = element_text(size=20),
            legend.title = element_text(size=20))

options(repr.plot.width=15, repr.plot.height=10)

unem_plot <- unem_prop %>% 
  ggplot(aes(x = unem,
             y = prop,
             fill = unem)) + 
  geom_col(alpha=0.7,
         color = "black") +
  geom_label(aes(label = label_text),
           fill="white",
           size =8,
           position = position_fill(vjust=0.3),
           color = "black")+
  xlab("Unem(0,1)") +
  ylab("Prop") +
  ggtitle("Unem Bar Graph Distribution") +
  scale_y_continuous(labels = scales::percent_format()) + 
  theme_minimal() +
  scale_fill_manual(values = c("darkblue", "#F5FFFA")) +
  sam

unem_plot

```


### Balancing the dataset
In order to increase the models ability to predict positive cases we create a more balanced dataset. We increase the proportion of positive cases for 'unem' from originally 3% to 13% by dropping 200000 rows of negative cases. 

```{r balancing the dataset}
set.seed(666)

N = 200000
df <- df[-sample(which(df$unem == "0"), N),]

# Plot balanced unem
unem_balanced <- df %>%
  group_by(unem) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

unem_balanced
```

## Logistic Regression 

Here we used 2015, 2016 and 2017 datasets to train a logistic regression model.

```{r}
# Set seed for reproducibility
set.seed(666)

# Crete an index
index <- createDataPartition(df$unem, 
                             p = .80, 
                             list = FALSE, 
                             times = 1)

# Create test and train data frames
train_set <- df[index,]
test_set <- df[-index,]
nrow(train_set)
nrow(test_set)

train.control <- trainControl(method = "cv", 
                              number = 5)
```

```{r}
model.glm <- train(unem ~ ., 
                   data = train_set, 
                   method ="glm", 
                   family = binomial,  
                   trControl = train.control)
summary(model.glm)
```
We get 'Warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred' which means that predicted probabilities of one or more observations in our data frame are indistinguishable from 0 or 1. There are three ways to handle this warning (https://www.statology.org/glm-fit-fitted-probabilities-numerically-0-or-1-occurred/):
1) Ignore it: In some cases, it doesn’t necessarily indicate that something is wrong with the logistic regression model. It just says that one or more observations in the data frame have predicted values indistinguishable from 0 or 1.
2) Increase the sample size: In other cases, this warning message appears when working with small data frames where there’s not enough data to provide a reliable model fit.
3) Remove outliers: In other cases, this error occurs when there are outliers in the original data frame and where only a small number of observations have fitted probabilities close to 0 or 1. 

We chose to ignore the warning message to test the model on the 2018 dataset.

Being female; having a home mortgage; being married; being widowed and a higher total income (incp_all) decreased the probability of being unemployed.
Having black, hispanic or native american ethinicity; working in job class Private; working Full-time, part year in the last year; working Part-time, part year in the last year; more weeks working in the last year; more usual hours per week increased the probability of being unemployed.

Some of the predictor coefficients don’t seem to make much sense, namely: working Full-time, part year in the last year; working Part-time, part year in the last year; more weeks working in the last year; more usual hours per week

The cause of this issue could be the following:
X² = 27.945 (Null Deviance - Residual Deviance) and 30 degrees of freedom ->
P-value associated with this Chi-Square statistic of 0.573351 which is more than .05 and indicates that the model is not highly useful.


```{r predictions on 2017 test set}
predictions.glm <- predict(model.glm, 
                           newdata = test_set, 
                           type = "prob")[,2]
head(predictions.glm)
```


```{r confusion matrix of 2017 test set}
predicted_class <- as.factor(ifelse(predictions.glm > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm <- confusionMatrix(predicted_class, 
                      test_set$unem)
cm
```
Nevertheless the accuracy of the model is 0.9349, which means that it correctly classified 93.49% of the observations in the test set. The 95% confidence interval for the accuracy ranges from 0.9305 to 0.939.
The p-value for the accuracy being greater than the NIR is less than 2.2e-16, which indicates that the model's performance is significantly better than random guessing.
The sensitivity of the model is 0.9430, which is the proportion of true positives out of all positive cases (i.e., TP/(TP+FN)). The specificity is 0.8817, which is the proportion of true negatives out of all negative cases (i.e., TN/(TN+FP)). 
Lastly, it seems like there is an issue with the 'Positive' Class value being 0. It may be worth investigating this issue further to ensure that the model is correctly classifying the positive class.

One possibility is that the imbalance between the positive and negative classes in the data is too extreme. If the positive class is rare or underrepresented, the model may struggle to correctly identify it, leading to a high number of false negatives and a low number of true positives.
Another possibility is that there is an issue with the data processing or feature engineering steps. It may be worth checking the data for outliers or other anomalies that could affect the model's performance.


```{r}
# ROC curve
pred.glm <- prediction(predictions.glm, 
                       test_set$unem)
perf.glm <- performance(pred.glm, 
                        measure = "tpr", 
                        x.measure = "fpr") 
par(mfrow = c(1,1))

plot(perf.glm, lty=1, 
     col="blue", 
     main = "Logit ROC Curve")
```

```{r}
# Area under the curve (AUC)
auc.glm <- performance(pred.glm, 
                       measure = "auc", 
                       x.measure = "fpr") 
auc.glm@y.values

# Save model
# saveRDS(model.glm, 
#         "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```
Our area under the ROC curve seems to be quite high (0,97) which indicates that our algorithm does a good job at ranking the test data.


Predicting 2018 data

We apply the same cleaning steps to the 2018 dataset as for the training data.
We filter rows where 'unem' has missing values and convert categorical variables to factor.

```{r}
df2018 <- df2018 %>% 
  filter(!is.na(unem)) %>%
  mutate(across(c("wbhaom",
                   "marstat",
                   "clslyr",
                   "ftptlyr", 
                   "female",
                   "forborn", 
                   "citizen",
                   "hprsmort", 
                   "unem",
                   "fulltimelw"), 
                 as.factor)) 
```

Then, we impute NAs with zeros '0'.

```{r NA imputation }
df2018[is.na(df2018)] <- 0
```

Now we test the logistic regression model which was trained with 2015-2017 data with the 2018 dataset.

```{r}
predictions.glm.2018 <- predict(model.glm, 
                           newdata = df2018, 
                           type = "prob")[,2]
head(predictions.glm.2018)
```
We create a confusion matrix for the predictions with 2018 data:

```{r confusion matrix}
predicted_class_2018 <- as.factor(ifelse(predictions.glm.2018 > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm2018 <- confusionMatrix(predicted_class_2018, 
                      df2018$unem)

length(predicted_class_2018)

cm2018
```
The model performance with 2018 data has remained about the same. The accuracy is 0.9414, which means that the model correctly classified 94.14% of the observations in the test set. 

Now we plot the ROC curve for the 2018 dataset:

```{r}
# ROC curve
pred.glm.2018 <- prediction(predictions.glm.2018, 
                       df2018$unem)
perf.glm.2018 <- performance(pred.glm.2018, 
                        measure = "tpr", 
                        x.measure = "fpr") 
par(mfrow = c(1,1))

plot(perf.glm.2018, 
     lty = 1, 
     col = "blue", 
     main = "Logit ROC Curve")
```

```{r}
# Area under the curve (AUC)
auc.glm.2018 <- performance(pred.glm.2018, 
                       measure = "auc", 
                       x.measure = "fpr") 
auc.glm.2018@y.values

# Save model
# saveRDS(model.glm, 
#         "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```
The area under the ROC curve with 2018 data has remained about the same. Our area under the ROC curve still seems to be quite high (0,97) which indicates that our algorithm does a good job at ranking the 2018 data, too.


## K-nearest neighbour

```{r}
# Read pre-trained model (if using the pre-trained model that is provided with the Google Drive link)
model.knn <- readRDS("knn_model.rds")

# Fit model and train
model.knn <- train(unem ~ ., 
                   data = train_set, 
                   method = "knn")


pred.knn <- predict(model.knn, 
                     newdata = test_set, 
                     type = "prob")[,2]

```

# Plot confusion matrix
```{r}
predicted_class_knn <- as.factor(
  ifelse(
    pred.knn > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm.knn <- confusionMatrix(predicted_class_knn, 
                          test_set$unem)
cm.knn
```
The accuracy of the model is 0.9102, which means that it correctly classified 91.02% of the observations in the test set.
The p-value for the accuracy being greater than the NIR is less than 2.2e-16, which indicates that the model's performance is significantly better than random guessing.
The sensitivity of the model is 0.9587 and the specificity is 0.5910.
Therefore the model which we trained with the k-nearest neighbour algorithm performs slightly weaker than the logistic regression model.

```{r}
# ROC curve
roc.knn <- prediction(pred.knn, 
                       test_set$unem)
perf.knn <- performance(roc.knn,
                        measure = "tpr", 
                        x.measure = "fpr") 

par(mfrow = c(1,1))

plot(perf.knn, 
     lty = 1, 
     col = "blue", 
     main = "K-nearest neighbour ROC curve")
```

```{r}
# Area under the curve (AUC)
auc.knn <- performance(roc.knn, 
                       measure = "auc", 
                       x.measure = "fpr") 
auc.knn@y.values

# Save model
#saveRDS(model.glm, "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```
The area under the ROC curve decreased slightly compared to the logit model. Our area under the ROC curve still seems to be quite high (0,94).

Now predict 2018 with knn:

```{r predict 2018}
pred.knn.2018 <- predict(model.knn, 
                     newdata = df2018, 
                     type = "prob")[,2]
length(pred.knn.2018)
length(df2018$unem)

```

# Plot confusion matrix
```{r}
predicted_class_knn_2018 <- as.factor(
  ifelse(
    pred.knn.2018 > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm.knn.2018 <- confusionMatrix(predicted_class_knn_2018, 
                          df2018$unem)
length(df2018$unem)

cm.knn.2018
```



```{r}
# ROC curve
roc.knn.2018 <- prediction(pred.knn.2018, 
                       df2018$unem)


perf.knn.2018 <- performance(roc.knn.2018,
                        measure = "tpr", 
                        x.measure = "fpr") 

par(mfrow = c(1,1))

plot(perf.knn.2018, 
     lty = 1, 
     col = "blue", 
     main = "K-nearest neighbour ROC curve")
```

```{r}
# Area under the curve (AUC)
auc.knn <- performance(roc.knn.2018, 
                       measure = "auc", 
                       x.measure = "fpr") 
auc.knn@y.values

# Save model
#saveRDS(model.glm, "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```

Our area under the ROC curve still seems to be quite high (XXXX). The area under the ROC curve decreased/increased slightly compared to the logit model. 


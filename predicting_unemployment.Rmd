---
title: "svm"
author: "Carolina Cornejo Castellano"
date: '`r Sys.Date()`'
output: pdf_document
---

# Libraries

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(caret) # for machine learning
library(naniar) # for better handling NA
library(fastDummies) # for 'dummifying' categorical variables
library(factoextra)
library(ROCR) #for plotting AUC
library(janitor) # for standardizing column names
library(MASS)
library(e1071)
library(pROC)
library(kernlab) # ksvm
library(doParallel) # for parallel programming
```

# Sourcing 2017 data and cleaning

```{r this is the training data}
source("cleaning_2017.R")
```

# Sourcing 2018 data and cleaning

```{r this is test data}
source("cleaning_2018.R")
```

# Sourcing 2016 data and cleaning

Following the recommendations of one of our lecturers, we tested the ksvm model of the `kernlab` package with other data, because the predictions on 2018 data were too good. So, we chose 2016 data.

```{r sourcing 2016 data to take it as a second test data}
source("cleaning_2016.R")
```


# SVM model

```{r create train and test datasets, fit the model and test}
# create a training set and a test set for predicting unem based on the 2017 data

set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017$unem, 
                                  p = 0.8, 
                                  list = FALSE)
train <- df_dummified_2017[trainIndex, ]
test <- df_dummified_2017[-trainIndex, ]

# fit the model
svm_model <- ksvm(unem ~ .,
    data = train,
    kernel = "rbfdot",
    C = 1,
    scale = FALSE
)

# testing the model
test_pred_2017 <- predict(svm_model, test)
```

```{r roc}
set.seed(234)
"ROC curve and AUC"

roc_2017 = roc(response = test$unem, 
               predictor = as.numeric(test_pred_2017))
roc_2017

control = 0
case = 1

plot.roc(roc_2017, 
         col = "red", 
         lwd = 3, 
         main = "ROC curve 2017 test data")
```
The latter means that R is assuming that the "control" level of the variable is the negative class and the "case" level is the positive class.

```{r confusion matrix based on 2017 test set predictions}
con_matrix_2017 <- confusionMatrix(test$unem, test_pred_2017)
con_matrix_2017
```
## Predicting unemployment in 2018 with SVM fit based on 2017 data

```{r 2018 unemployment prediction}
test_pred_2018 <- predict(svm_model, 
                          newdata = df_dummified_2018)
```

```{r AUC}
# Calculate AUC score
auc_score_2018 <- auc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Calculate ROC curve
roc_2018 <- roc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Plot ROC curve and AUC
plot(roc_2018, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2018")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2018, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r confusion matrix of 2018 predictions}
con_matrix_2018 <- confusionMatrix(df_dummified_2018$unem, test_pred_2018)
con_matrix_2018
```


## (reverse) Predicting unemployment in 2016 with SVM fit based on 2017 data

```{r 2016 unemployment prediction}
test_pred_2016 <- predict(svm_model, 
                          newdata = df_dummified_2016)
```

```{r AUC}
# Calculate AUC score
auc_score_2016 <- auc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Calculate ROC curve
roc_2016 <- roc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Plot ROC curve and AUC
plot(roc_2016, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2016")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2016, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r}
# Change the level names of the predicted values
levels(test_pred_2016) <- c("0", "1")

# Calculate the confusion matrix
conf_matrix_2016 <- confusionMatrix(df_dummified_2016$unem, test_pred_2016)
conf_matrix_2016
```


---
title: "Predicting Unemployment"
author: "Carolina Cornejo Castellano, Juan D Mendez, Jasmin Huynh"
date: '`r Sys.Date()`'
output: pdf_document
---

# Libraries

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(caret) # for machine learning
library(naniar) # for better handling NA
library(fastDummies) # for 'dummifying' categorical variables
library(factoextra)
library(ROCR) #for plotting AUC
library(janitor) # for standardizing column names
library(MASS)
library(e1071)
library(pROC)
library(kernlab) # ksvm
library(doParallel) # for parallel programming
library(klaR)
```

## Logistic Regression using 2015, 2016 and 2017 datasets

# Read datasets, select columns and filter
We use three datasets in order to obtain a larger training dataset. We manually selected variables and filtered keeping only people who are older than 16. Then we drop rows where negative values where present. This was the case in the columns 'incp_all' and 'hrearn'. 
```{r}
df <- read.csv("~/SRM II/Challenge/cps_2017.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)

df1 <- read.csv("~/SRM II/Challenge/cps_2016.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)

df2 <- read.csv("~/SRM II/Challenge/cps_2015.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)


```

Next we combine the three datasets to one single dataframe 'df3'
```{r}
df3 <- bind_rows(df,df1, df2) %>% filter(!is.na(unem))
  

summary(is.na(df3))
```

# Impute missing data
We impute NAs with zeros '0'. 
```{r}
df3[is.na(df3)] <- 0
summary(is.na(df3))
```
# Transform to factor
```{r}
str(df3)
df3 <- df3 %>% mutate(across(c("wbhaom","marstat","clslyr","ftptlyr", "female","forborn", "citizen","hprsmort", "unem","fulltimelw"), as.factor))

```

# Plot proportion of dependent variable 'unem'
We have a imbalanced dataset of only 3% postive cases for the dependent variable.
```{r}
options(warn=-1)
sam <- theme(plot.background = element_rect(fill="#F5FFFA",color = "darkblue"),
             plot.title = element_text(size=25, hjust=.5),
             axis.title.x = element_text(size=25, color = "black"),
              axis.title.y = element_text(size=25, color = "black"),
             axis.text.x = element_text(size=20),
             axis.text.y = element_text(size=20),
            legend.position = "top",
            legend.text = element_text(size=20),
            legend.title = element_text(size=20))

unem <- df3 %>%
group_by(unem) %>%
summarise(n = n())%>%
mutate(prop = n / sum(n)) %>%
ungroup()%>%
mutate(label_text = str_glue("n:{n} \n prop:{scales::percent(prop)}"))

unem

options(repr.plot.width=15, repr.plot.height=10)
unem1 <- unem %>% ggplot(aes(x = unem,y = prop,fill = unem)) + 
geom_col(alpha=0.7,color="black") +
geom_label(aes(label=label_text),fill="white",size =8,position=position_fill(vjust=0.3),color = "black",size = 10)+
xlab("Unem(0,1)") +
ylab("Prop") +
ggtitle("Unem Bar Graph Distribution")+
scale_y_continuous(labels = scales::percent_format())+
theme_minimal()+
sam
unem1

```
# Balancing the dataset
In order to increase the models ability to predict positve cases we create a more balanced dataset. We increase the proportion of positive cases for 'unem' from originally 3% to 13% be dropping 200000 rows of negative cases. 
```{r}
N= 200000
df3 <- df3[-sample(which(df3$unem == "0"), N),]

# Plot balanced unem
balanced_unem <- df3 %>%
group_by(unem) %>%
summarise(n = n())%>%
mutate(prop = n / sum(n)) %>%
ungroup()%>%
mutate(label_text = str_glue("n:{n} \n prop:{scales::percent(prop)}"))

balanced_unem

```
## Fit logistic regression model
```{r}
# Set seed for reproducibility
set.seed(666)

# Crete an index
index <- createDataPartition(df3$unem, p=.80, list=FALSE, times=1)

# Create test and train data frames
train_set <- df3[index,]
test_set <- df3[-index,]
nrow(train_set)
nrow(test_set)

train.control <- trainControl(method="cv", number=5)
```

```{r}
model.glm <- train(unem ~ ., 
                   data=train_set, 
                   method="glm", 
                   family=binomial,  
                   trControl=train.control)
summary(model.glm)
```


```{r}
predictions.glm <- predict(model.glm, newdata = test_set, type = "prob")[,2]
head(predictions.glm)
```

# Plot confusion matrix
```{r}
predicted_class <- as.factor(ifelse(predictions.glm > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm <- confusionMatrix(predicted_class, test_set$unem)
cm
```


```{r}
# ROC curve
pred.glm <- prediction(predictions.glm, test_set$unem)
perf.glm <- performance(pred.glm, measure = "tpr", x.measure = "fpr") 
par(mfrow = c(1,1))
plot(perf.glm, lty=1, col="blue", main = "Logit ROC Curve")
```

```{r}
# Area under the curve (AUC)
auc.glm <- performance(pred.glm, measure = "auc", x.measure = "fpr") 
auc.glm@y.values

# Save model
saveRDS(model.glm, "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```






## Support Vector Machine using 2017 dataset for training

# Sourcing 2017 data and cleaning

```{r this is the training data}
source("cleaning_2017.R")
```

# Sourcing 2018 data and cleaning

```{r this is test data}
source("cleaning_2018.R")
```

# Sourcing 2016 data and cleaning

Following the recommendations of one of our lecturers, we tested the ksvm model of the `kernlab` package with other data, because the predictions on 2018 data were too good. So, we chose 2016 data.

```{r sourcing 2016 data to take it as a second test data}
source("cleaning_2016.R")
```


# SVM model

```{r create train and test datasets, fit the model and test}
# create a training set and a test set for predicting unem based on the 2017 data

set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017$unem, 
                                  p = 0.8, 
                                  list = FALSE)
train <- df_dummified_2017[trainIndex, ]
test <- df_dummified_2017[-trainIndex, ]

# fit the model
svm_model <- ksvm(unem ~ .,
    data = train,
    kernel = "rbfdot",
    C = 1,
    scale = FALSE
)

# testing the model
test_pred_2017 <- predict(svm_model, test)
```

```{r roc}
set.seed(234)
"ROC curve and AUC"

roc_2017 = roc(response = test$unem, 
               predictor = as.numeric(test_pred_2017))
roc_2017

control = 0
case = 1

plot.roc(roc_2017, 
         col = "red", 
         lwd = 3, 
         main = "ROC curve 2017 test data")
```
The latter means that R is assuming that the "control" level of the variable is the negative class and the "case" level is the positive class.

```{r confusion matrix based on 2017 test set predictions}
con_matrix_2017 <- confusionMatrix(test$unem, test_pred_2017)
con_matrix_2017
```
## Predicting unemployment in 2018 with SVM fit based on 2017 data

```{r 2018 unemployment prediction}
test_pred_2018 <- predict(svm_model, 
                          newdata = df_dummified_2018)
```

```{r AUC}
# Calculate AUC score
auc_score_2018 <- auc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Calculate ROC curve
roc_2018 <- roc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Plot ROC curve and AUC
plot(roc_2018, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2018")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2018, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r confusion matrix of 2018 predictions}
con_matrix_2018 <- confusionMatrix(df_dummified_2018$unem, test_pred_2018)
con_matrix_2018
```


## (reverse) Predicting unemployment in 2016 with SVM fit based on 2017 data

```{r 2016 unemployment prediction}
test_pred_2016 <- predict(svm_model, 
                          newdata = df_dummified_2016)
```

```{r AUC}
# Calculate AUC score
auc_score_2016 <- auc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Calculate ROC curve
roc_2016 <- roc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Plot ROC curve and AUC
plot(roc_2016, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2016")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2016, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r}
# Change the level names of the predicted values
levels(test_pred_2016) <- c("0", "1")

# Calculate the confusion matrix
conf_matrix_2016 <- confusionMatrix(df_dummified_2016$unem, test_pred_2016)
conf_matrix_2016
```



# Naive Bayes

```{r}
set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017$unem, p = 0.8, list = FALSE)
train <- df_dummified_2017[trainIndex, ]
test <- df_dummified_2017[-trainIndex, ]
```

```{r}
ctrl <- trainControl(method = "boot632",
                     number = 10,
                     classProbs = TRUE,
                     allowParallel = TRUE)
```

```{r}
weights <- ifelse(train$unem == 1, 1, 3)
nb_model <- train(unem ~ ., 
                  data = train, 
                  method = "nb", 
                  weights = weights,
                  trControl = ctrl)
```

```{r}
test_pred <- predict(nb_model, newdata = test)
```

```{r}
conf_matrix_nb_2017 <- confusionMatrix(test$unem, test_pred)
conf_matrix_nb_2017
```


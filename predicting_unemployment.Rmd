---
title: "Predicting Unemployment"
author: "Carolina Cornejo Castellano, Juan D Mendez, Jasmin Huynh"
date: '`r Sys.Date()`'
output: pdf_document
---

# Libraries

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(caret) # for machine learning
library(naniar) # for better handling NA
library(fastDummies) # for 'dummifying' categorical variables
library(factoextra)
library(ROCR) #for plotting AUC
library(janitor) # for standardizing column names
library(MASS)
library(e1071)
library(pROC)
library(kernlab) # ksvm
library(doParallel) # for parallel programming
library(klaR)
```

## Logistic Regression using 2015, 2016 and 2017 datasets

# Read datasets, select columns and filter
We use three datasets in order to obtain a larger training dataset. We manually selected variables and filtered keeping only people who are older than 16. Then we drop rows where negative values where present. This was the case in the columns 'incp_all' and 'hrearn'. 
```{r}
df <- read.csv("~/SRM II/Challenge/cps_2017.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)

df1 <- read.csv("~/SRM II/Challenge/cps_2016.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)

df2 <- read.csv("~/SRM II/Challenge/cps_2015.csv", sep=";") %>%
  dplyr::select(
    age,
    female,
    wbhaom,
    forborn,
    citizen,
    hprsmort,
    marstat,
    wkslyr,
    clslyr,
    ftptlyr,
    unem,
    uhours,
    hours,
    fulltimelw,
    incp_all,
    hrearn,
    hrwage
  ) %>%
  mutate_all(~ ifelse(. == "", NA, .)) %>%
  filter(age >= 16 & age <= 65, incp_all >= 0, hrearn >= 0)


```

Next we combine the three datasets to one single dataframe 'df3'
```{r}
df3 <- bind_rows(df,df1, df2) %>% filter(!is.na(unem))

summary(is.na(df3))
```

# Impute missing data
We impute NAs with zeros '0'. 
```{r}
df3[is.na(df3)] <- 0
summary(is.na(df3))
```
# Transform to factor
```{r}
str(df3)
df3 <- df3 %>% mutate(across(c("wbhaom","marstat","clslyr","ftptlyr", "female","forborn", "citizen","hprsmort", "unem","fulltimelw"), as.factor))

```

# Plot proportion of dependent variable 'unem'
We have a imbalanced dataset of only 3% postive cases for the dependent variable.
```{r}
options(warn=-1)
sam <- theme(plot.background = element_rect(fill="#F5FFFA",color = "darkblue"),
             plot.title = element_text(size=25, hjust=.5),
             axis.title.x = element_text(size=25, color = "black"),
              axis.title.y = element_text(size=25, color = "black"),
             axis.text.x = element_text(size=20),
             axis.text.y = element_text(size=20),
            legend.position = "top",
            legend.text = element_text(size=20),
            legend.title = element_text(size=20))

unem <- df3 %>%
group_by(unem) %>%
summarise(n = n())%>%
mutate(prop = n / sum(n)) %>%
ungroup()%>%
mutate(label_text = str_glue("n:{n} \n prop:{scales::percent(prop)}"))

unem

options(repr.plot.width=15, repr.plot.height=10)
unem1 <- unem %>% ggplot(aes(x = unem,y = prop,fill = unem)) + 
geom_col(alpha=0.7,color="black") +
geom_label(aes(label=label_text),fill="white",size =8,position=position_fill(vjust=0.3),color = "black",size = 10)+
xlab("Unem(0,1)") +
ylab("Prop") +
ggtitle("Unem Bar Graph Distribution")+
scale_y_continuous(labels = scales::percent_format())+
theme_minimal()+
sam
unem1

```
# Balancing the dataset
In order to increase the models ability to predict positve cases we create a more balanced dataset. We increase the proportion of positive cases for 'unem' from originally 3% to 13% be dropping 200000 rows of negative cases. 
```{r}
N= 200000
df3 <- df3[-sample(which(df3$unem == "0"), N),]

# Plot balanced unem
balanced_unem <- df3 %>%
group_by(unem) %>%
summarise(n = n())%>%
mutate(prop = n / sum(n)) %>%
ungroup()%>%
mutate(label_text = str_glue("n:{n} \n prop:{scales::percent(prop)}"))

balanced_unem

```
## Fit logistic regression model
```{r}
# Set seed for reproducibility
set.seed(666)

# Crete an index
index <- createDataPartition(df3$unem, p=.80, list=FALSE, times=1)

# Create test and train data frames
train_set <- df3[index,]
test_set <- df3[-index,]
nrow(train_set)
nrow(test_set)

train.control <- trainControl(method="cv", number=5)
```

```{r}
model.glm <- train(unem ~ ., 
                   data=train_set, 
                   method="glm", 
                   family=binomial,  
                   trControl=train.control)
summary(model.glm)
```
Decrease prop: Being female; having a home mortgage; being married; incp_all
Increase prop: Having black, hispanic or native american ethinicity; wkslyr; working in job class Private; working Full-time, part year in the last year, working Part-time, part year in the last year; usual hours per week;


```{r}
predictions.glm <- predict(model.glm, newdata = test_set, type = "prob")[,2]
head(predictions.glm)
```

# Plot confusion matrix
```{r}
predicted_class <- as.factor(ifelse(predictions.glm > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm <- confusionMatrix(predicted_class, test_set$unem)
cm
```


```{r}
# ROC curve
pred.glm <- prediction(predictions.glm, test_set$unem)
perf.glm <- performance(pred.glm, measure = "tpr", x.measure = "fpr") 
par(mfrow = c(1,1))
plot(perf.glm, lty=1, col="blue", main = "Logit ROC Curve")
```

```{r}
# Area under the curve (AUC)
auc.glm <- performance(pred.glm, measure = "auc", x.measure = "fpr") 
auc.glm@y.values

# Save model
saveRDS(model.glm, "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```

## K-nearest neighbour

```{r}
# Read pre-trained model (if using the pre-trained model that is provided with the Google Drive link)
#model_knn <- readRDS("knn_model.rds")

# Fit model and train
model_knn <- train(unem ~ ., data = train_set, method = "knn")


model_knn <- predict(model_knn, newdata = test_set, type = "prob")[,2]
head(model_knn)

```

# Plot confusion matrix
```{r}
predicted_class_knn <- as.factor(ifelse(model_knn > 0.5, 1, 0))

# Create confusion matrix and calculate performance measures
cm_knn <- confusionMatrix(predicted_class_knn, test_set$unem)
cm_knn
```
```{r}
# ROC curve
pred.knn <- prediction(model_knn, test_set$unem)
perf.knn <- performance(pred.knn, measure = "tpr", x.measure = "fpr") 
par(mfrow = c(1,1))
plot(perf.knn, lty=1, col="blue", main = "K-nearest neighbour ROC curve")
```

```{r}
# Area under the curve (AUC)
auc.knn <- performance(pred.knn, measure = "auc", x.measure = "fpr") 
auc.knn@y.values

# Save model
#saveRDS(model.glm, "log_model.rds")
# read saved pre-trained model to skip training
# model.glm <- readRDS("log_model.rds")

```



## Support Vector Machine using 2017 dataset for training

# Sourcing 2017 data and cleaning

```{r this is the training data}
source("cleaning_2017.R")
```

# Sourcing 2018 data and cleaning

```{r this is test data}
source("cleaning_2018.R")
```

# Sourcing 2016 data and cleaning

Following the recommendations of one of our lecturers, we tested the ksvm model of the `kernlab` package with other data, because the predictions on 2018 data were too good. So, we chose 2016 data.

```{r sourcing 2016 data to take it as a second test data}
source("cleaning_2016.R")
```


# SVM model

```{r create train and test datasets, fit the model and test}
# create a training set and a test set for predicting unem based on the 2017 data

set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017$unem, 
                                  p = 0.8, 
                                  list = FALSE)
train <- df_dummified_2017[trainIndex, ]
test <- df_dummified_2017[-trainIndex, ]

# fit the model
svm_model <- ksvm(unem ~ .,
    data = train,
    kernel = "rbfdot",
    C = 1,
    scale = FALSE
)

# testing the model
test_pred_2017 <- predict(svm_model, test)

To plot a ROC curve for your SVM model, you first need to calculate the true positive rate (TPR) and false positive rate (FPR) for different threshold values. Here's an example code to do that:

```{r}
library(pROC)

# calculate TPR and FPR for different threshold values
roc_data <- roc(test$unem, test_pred_2017)

# plot the ROC curve
plot(roc_data, main = "ROC Curve for SVM Model", 
     xlab = "False Positive Rate", ylab = "True Positive Rate")
```

This code uses the `pROC` package to calculate the ROC curve and plot it. The `roc()` function takes the true class labels (`test$unem`) and predicted probabilities (`test_pred_2017`) as inputs, and returns a `roc` object. You can then use the `plot()` function to plot the ROC curve, with the FPR on the x-axis and TPR on the y-axis. The resulting plot will show the performance of your SVM model in terms of its ability to discriminate between positive and negative cases.
```

```{r 2017 test set AUC}
set.seed(234)
"ROC curve and AUC"

roc_2017 = roc(response = test$unem, 
               predictor = as.numeric(test_pred_2017))
roc_2017

control = 0
case = 1

plot.roc(roc_2017, 
         col = "red", 
         lwd = 3, 
         main = "ROC curve 2017 test data")
```
The latter means that R is assuming that the "control" level of the variable is the negative class and the "case" level is the positive class.

```{r 2017 test set confusion matrix}
con_matrix_2017 <- confusionMatrix(test$unem, test_pred_2017)
con_matrix_2017
```
## Predicting unemployment in 2018 with SVM fit based on 2017 data

```{r 2018 unemployment prediction}
test_pred_2018 <- predict(svm_model, 
                          newdata = df_dummified_2018)
```

```{r 2018 AUC}
# Calculate AUC score
auc_score_2018 <- auc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Calculate ROC curve
roc_2018 <- roc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Plot ROC curve and AUC
plot(roc_2018, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2018")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2018, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r 2018 confusion matrix}
con_matrix_2018 <- confusionMatrix(df_dummified_2018$unem, test_pred_2018)
con_matrix_2018
```


## Predicting unemployment in 2016 with SVM fit based on 2017 data (reverse)

```{r 2016 unemployment prediction}
test_pred_2016 <- predict(svm_model, 
                          newdata = df_dummified_2016)
```

```{r 2016 AUC}
# Calculate AUC score
auc_score_2016 <- auc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Calculate ROC curve
roc_2016 <- roc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Plot ROC curve and AUC
plot(roc_2016, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2016")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2016, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r 2016 confusion matrix}
# Change the level names of the predicted values
levels(test_pred_2016) <- c("0", "1")

# Calculate the confusion matrix
conf_matrix_2016 <- confusionMatrix(df_dummified_2016$unem, test_pred_2016)
conf_matrix_2016
```
So far, the models have ran somewhat faster. However, the following ones take way more time computing. For that reason, starting from this point we will follow two approaches:

* Performing Principal Component Analysis (PCA), in order to do feature selection
* Parallel programming.

# Principal Component Analysis

```{r pca}
# turning "unem" into numeric again, for it to be included in the PCA. Will be returned into factor again later:
df_dummified_2017$unem <- as.numeric(df_dummified_2017$unem)

# PCA:
pca <- prcomp(df_dummified_2017, 
              scale = T) 
summary(pca)
```

```{#r pca}
pca <- prcomp(df_dummified_2017[, !(names(df_dummified_2017) %in% c("unem"))], 
              scale = T) 
summary(pca)
```

```{r scree plot of PCA}
fviz_screeplot(pca, addlabels = TRUE)
```
```{r identify the top contributing variables according to PCA}
# extract the loadings for the first 42 PCs, that explain up to 99% of variability
pca_loadings <- pca$rotation[, 1:42]

# compute the contribution of each variable to each PC
pca_var_contribution <- abs(pca_loadings) / sqrt(colSums(df_dummified_2017^2))

# compute the total contribution of each variable across all 23 PCs
pca_total_contribution <- rowSums(pca_var_contribution)

# identify the top contributing variables
pca_top_vars <- names(sort(pca_total_contribution, decreasing = TRUE)[1:42])
pca_top_vars

# as can be seen, "unem" is not included, but as it is our target variable, we should include it:
pca_top_vars <- c(pca_top_vars, "unem")
pca_top_vars
```

```{r select priotitized variables in 2017 data}
df_dummified_2017_pca_vars <- df_dummified_2017[, pca_top_vars]

df_dummified_2017_pca_vars$unem <- factor(make.names(df_dummified_2017_pca_vars$unem))

dim(df_dummified_2017_pca_vars)

```
Now, we went from 87 to 43 variables. This is the dataset we are going to work with from now on.

# Naive Bayes

```{#r}
ctrl <- trainControl(method = "loocv",
                      number = 10,
                      classProbs = TRUE,
                      allowParallel = TRUE)
```

```{#r}
weights <- ifelse(train$unem == 1, 1, 3)
nb_model <- train(unem ~ .,
                   data = train,
                   method = "nb",
                   weights = weights,
                   trControl = ctrl)
```

```{#r}
test_pred <- predict(nb_model, newdata = test)
```

```{#r}
conf_matrix_nb_2017 <- confusionMatrix(test$unem, test_pred)
conf_matrix_nb_2017
```

# Random Forest

```{r parallelizing}
library(doParallel)

# Register the cores or workers
num_cores <- detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
```

```{r}
set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017_pca_vars$unem, 
                                  p = 0.8, list = FALSE)
train <- df_dummified_2017_pca_vars[trainIndex, ]
test <- df_dummified_2017_pca_vars[-trainIndex, ]
```

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10,
                     classProbs = T,
                     summaryFunction=twoClassSummary,
                     verboseIter = T,
                     allowParallel = TRUE)
```

```{r start timer}
start_time <- system.time()
```

```{r}
rf_model <- train(unem ~ ., 
                  data = train,
                  method = "rf",   
                  preProc = c('scale','center'),
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl)
plot(rf_model)

# rf_prob = predict(rf_model, test, type="prob")
# rf_prediction_2017 <- as.factor(ifelse(rf_prob[,2] > 0.1, "Yes", "No"))
# 
# confusionMatrix(rf_prediction_2017, test$unem)$table
# confusionMatrix(rf_prediction_2017, test$unem)$overall[1:2]
```

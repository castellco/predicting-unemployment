---
title: "svm"
author: "Carolina Cornejo Castellano"
date: '`r Sys.Date()`'
output: pdf_document
---

# Libraries

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(caret) # for machine learning
library(naniar) # for better handling NA
library(fastDummies) # for 'dummifying' categorical variables
library(factoextra)
library(ROCR) #for plotting AUC
library(janitor) # for standardizing column names
library(MASS)
library(e1071)
library(pROC)
library(kernlab) # ksvm
library(doParallel) # for parallel programming
library(klaR)
```

# Sourcing 2017 data and cleaning

```{r this is the training data}
source("cleaning_2017.R")
```

# Sourcing 2018 data and cleaning

```{r this is test data}
source("cleaning_2018.R")
```

# Sourcing 2016 data and cleaning

Following the recommendations of one of our lecturers, we tested the ksvm model of the `kernlab` package with other data, because the predictions on 2018 data were too good. So, we chose 2016 data.

```{r sourcing 2016 data to take it as a second test data}
source("cleaning_2016.R")
```


# SVM model

```{r create train and test datasets, fit the model and test}
# create a training set and a test set for predicting unem based on the 2017 data

set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017$unem, 
                                  p = 0.8, 
                                  list = FALSE)
train <- df_dummified_2017[trainIndex, ]
test <- df_dummified_2017[-trainIndex, ]

# fit the model
svm_model <- ksvm(unem ~ .,
    data = train,
    kernel = "rbfdot",
    C = 1,
    scale = FALSE
)

# testing the model
test_pred_2017 <- predict(svm_model, test)
```

```{r 2017 test set AUC}
set.seed(234)
"ROC curve and AUC"

roc_2017 = roc(response = test$unem, 
               predictor = as.numeric(test_pred_2017))
roc_2017

control = 0
case = 1

plot.roc(roc_2017, 
         col = "red", 
         lwd = 3, 
         main = "ROC curve 2017 test data")
```
The latter means that R is assuming that the "control" level of the variable is the negative class and the "case" level is the positive class.

```{r 2017 test set confusion matrix}
con_matrix_2017 <- confusionMatrix(test$unem, test_pred_2017)
con_matrix_2017
```
## Predicting unemployment in 2018 with SVM fit based on 2017 data

```{r 2018 unemployment prediction}
test_pred_2018 <- predict(svm_model, 
                          newdata = df_dummified_2018)
```

```{r 2018 AUC}
# Calculate AUC score
auc_score_2018 <- auc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Calculate ROC curve
roc_2018 <- roc(df_dummified_2018$unem, as.numeric(test_pred_2018))

# Plot ROC curve and AUC
plot(roc_2018, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2018")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2018, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r 2018 confusion matrix}
con_matrix_2018 <- confusionMatrix(df_dummified_2018$unem, test_pred_2018)
con_matrix_2018
```


## Predicting unemployment in 2016 with SVM fit based on 2017 data (reverse)

```{r 2016 unemployment prediction}
test_pred_2016 <- predict(svm_model, 
                          newdata = df_dummified_2016)
```

```{r 2016 AUC}
# Calculate AUC score
auc_score_2016 <- auc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Calculate ROC curve
roc_2016 <- roc(df_dummified_2016$unem, as.numeric(test_pred_2016))

# Plot ROC curve and AUC
plot(roc_2016, 
     col = "red", 
     lwd = 3, 
     main = "ROC curve 2016")
legend("bottomright", 
       legend = paste("AUC = ", round(auc_score_2016, 2)),
       col = "red", 
       lty = 1, 
       cex = 0.8)
```

```{r 2016 confusion matrix}
# Change the level names of the predicted values
levels(test_pred_2016) <- c("0", "1")

# Calculate the confusion matrix
conf_matrix_2016 <- confusionMatrix(df_dummified_2016$unem, test_pred_2016)
conf_matrix_2016
```
So far, the models have ran somewhat faster. However, the following ones take way more time computing. For that reason, starting from this point we will follow two approaches:

* Performing Principal Component Analysis (PCA), in order to do feature selection
* Parallel programming.

# Principal Component Analysis

```{r pca}
# turning "unem" into numeric again, for it to be included in the PCA. Will be returned into factor again later:
df_dummified_2017$unem <- as.numeric(df_dummified_2017$unem)

# PCA:
pca <- prcomp(df_dummified_2017, 
              scale = T) 
summary(pca)
```

```{#r pca}
pca <- prcomp(df_dummified_2017[, !(names(df_dummified_2017) %in% c("unem"))], 
              scale = T) 
summary(pca)
```

```{r scree plot of PCA}
fviz_screeplot(pca, addlabels = TRUE)
```
```{r identify the top contributing variables according to PCA}
# extract the loadings for the first 42 PCs, that explain up to 99% of variability
pca_loadings <- pca$rotation[, 1:42]

# compute the contribution of each variable to each PC
pca_var_contribution <- abs(pca_loadings) / sqrt(colSums(df_dummified_2017^2))

# compute the total contribution of each variable across all 23 PCs
pca_total_contribution <- rowSums(pca_var_contribution)

# identify the top contributing variables
pca_top_vars <- names(sort(pca_total_contribution, decreasing = TRUE)[1:42])
pca_top_vars

# as can be seen, "unem" is not included, but as it is our target variable, we should include it:
pca_top_vars <- c(pca_top_vars, "unem")
pca_top_vars
```

```{r select priotitized variables in 2017 data}
df_dummified_2017_pca_vars <- df_dummified_2017[, pca_top_vars]

df_dummified_2017_pca_vars$unem <- factor(make.names(df_dummified_2017_pca_vars$unem))

dim(df_dummified_2017_pca_vars)

```
Now, we went from 87 to 43 variables. This is the dataset we are going to work with from now on.

# Naive Bayes

```{#r}
ctrl <- trainControl(method = "loocv",
                      number = 10,
                      classProbs = TRUE,
                      allowParallel = TRUE)
```

```{#r}
weights <- ifelse(train$unem == 1, 1, 3)
nb_model <- train(unem ~ .,
                   data = train,
                   method = "nb",
                   weights = weights,
                   trControl = ctrl)
```

```{#r}
test_pred <- predict(nb_model, newdata = test)
```

```{#r}
conf_matrix_nb_2017 <- confusionMatrix(test$unem, test_pred)
conf_matrix_nb_2017
```

# Random Forest

```{r}
set.seed(666)

trainIndex <- createDataPartition(df_dummified_2017_pca_vars$unem, 
                                  p = 0.8, list = FALSE)
train <- df_dummified_2017_pca_vars[trainIndex, ]
test <- df_dummified_2017_pca_vars[-trainIndex, ]
```

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, #10-fold cross validation
                     classProbs = T, # using classification, in classification queremos probabilities
                     summaryFunction=twoClassSummary, # to maximize the AUC, not the accuracy. Accuracy is bueno cuando tienes un balanced data
                     verboseIter = T)
```


```{r}
rf_model <- train(unem ~ ., 
                  data = train,
                  method = "rf",   
                  preProc = c('scale','center'),
                  tuneLength = 10,
                  metric = "ROC",
                  trControl = ctrl)
plot(rf_model)

rf_prob = predict(rf_model, test, type="prob")
rf_prediction_2017 <- as.factor(ifelse(rf_prob[,2] > 0.1, "Yes", "No"))

confusionMatrix(rf_prediction_2017, test$unem)$table
confusionMatrix(rf_prediction_2017, test$unem)$overall[1:2]
```



